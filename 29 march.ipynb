{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd6912e-0f09-4cdd-9fee-e93c44df3eba",
   "metadata": {},
   "source": [
    "Certainly! Lasso Regression, or L1 regularization, is a linear regression technique used in statistics and machine learning. It's a regularization method that adds a penalty term to the linear regression cost function, encouraging the model to prefer simpler models with fewer features. The term \"Lasso\" stands for \"Least Absolute Shrinkage and Selection Operator.\"\n",
    "\n",
    "Here's a breakdown of Lasso Regression and how it differs from other regression techniques:\n",
    "\n",
    "### Lasso Regression:\n",
    "\n",
    "1. **Objective Function:**\n",
    "   - The objective function in Lasso Regression is the sum of the squared error term (ordinary least squares) plus the absolute value of the coefficients multiplied by a regularization parameter (alpha) times the sum of absolute values of coefficients.\n",
    "\n",
    "2. **Regularization Term:**\n",
    "   - The regularization term in Lasso Regression is \\(\\alpha \\times \\sum_{i=1}^{n} |\\beta_i|\\), where \\(\\alpha\\) is the regularization parameter and \\(\\beta_i\\) represents the coefficients of the features.\n",
    "\n",
    "3. **Sparsity:**\n",
    "   - One of the main features of Lasso Regression is that it tends to produce sparse models, meaning it encourages some of the coefficients to be exactly zero. This can lead to feature selection, as some features become irrelevant for the model.\n",
    "\n",
    "4. **Feature Selection:**\n",
    "   - Lasso can be useful when dealing with a dataset with many features, as it automatically selects a subset of the most relevant features, effectively performing feature selection.\n",
    "\n",
    "### Differences from Other Regression Techniques:\n",
    "\n",
    "1. **Ridge Regression vs. Lasso:**\n",
    "   - Ridge Regression also adds a regularization term, but it uses the squared values of the coefficients. Unlike Lasso, Ridge tends to shrink coefficients towards zero but rarely makes them exactly zero. It does not perform feature selection.\n",
    "\n",
    "2. **Ordinary Least Squares (OLS):**\n",
    "   - OLS does not have a regularization term. It aims to minimize the sum of squared errors, which might lead to overfitting, especially when dealing with a high-dimensional dataset.\n",
    "\n",
    "3. **Elastic Net Regression:**\n",
    "   - Elastic Net combines L1 and L2 regularization terms. It has both the feature selection property of Lasso and the regularization strength of Ridge, providing a balance between the two.\n",
    "\n",
    "4. **Sparsity vs. Shrinkage:**\n",
    "   - Lasso primarily focuses on sparsity, making some coefficients exactly zero. Ridge, on the other hand, aims for shrinkage, reducing the magnitude of all coefficients but rarely setting any to zero.\n",
    "\n",
    "In summary, Lasso Regression is a useful technique when dealing with high-dimensional datasets and you suspect that not all features are relevant. It can help prevent overfitting and provide a more interpretable model by automatically selecting a subset of important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2d623c-dd02-4860-8e02-ee43905369ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6945b491-c903-4605-ab4f-8ab0b7b1b285",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression in feature selection lies in its ability to automatically select a subset of relevant features by inducing sparsity in the model. Here are the key advantages:\n",
    "\n",
    "1. **Automatic Feature Selection:**\n",
    "   - Lasso Regression's regularization term (L1 penalty) includes the absolute values of the coefficients. As a result, during the optimization process, some coefficients are driven exactly to zero. This leads to automatic feature selection, effectively excluding irrelevant or less important features from the model.\n",
    "\n",
    "2. **Simplicity and Interpretability:**\n",
    "   - The sparsity induced by Lasso makes the model simpler and more interpretable. By having fewer features with non-zero coefficients, it becomes easier to understand and communicate the impact of each selected feature on the target variable.\n",
    "\n",
    "3. **Reduced Overfitting:**\n",
    "   - Lasso helps prevent overfitting, especially in situations where the number of features is much larger than the number of observations. By penalizing the absolute values of coefficients, Lasso discourages the model from fitting noise in the data, resulting in a more generalized and robust model.\n",
    "\n",
    "4. **Handling Multicollinearity:**\n",
    "   - Lasso Regression is effective in dealing with multicollinearity, a situation where predictor variables are highly correlated. When features are highly correlated, ordinary least squares (OLS) regression can have unstable coefficients. Lasso tends to select one feature from a group of correlated features and assign non-zero coefficients to the selected ones, helping to address multicollinearity issues.\n",
    "\n",
    "5. **Improving Model Performance:**\n",
    "   - Feature selection with Lasso can lead to improved model performance, especially when dealing with datasets with a large number of irrelevant or redundant features. By focusing on a subset of important features, the model becomes more efficient and may generalize better to new, unseen data.\n",
    "\n",
    "6. **Facilitating Model Interpretation and Implementation:**\n",
    "   - In various fields, interpretability is crucial. Lasso's feature selection property not only helps in understanding the key drivers of the model but also facilitates the implementation of simpler models in real-world scenarios.\n",
    "\n",
    "While Lasso Regression has these advantages, it's important to note that the choice between Lasso and other regularization techniques, such as Ridge Regression or Elastic Net, depends on the specific characteristics of the dataset and the goals of the analysis. In situations where feature sparsity and automatic selection are important, Lasso is a valuable tool in the data scientist's toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abbc2ea-8a19-4ce8-920d-3ede2d91a002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb1ec881-d970-44e2-9c16-cd907ed8c705",
   "metadata": {},
   "source": [
    "Interpreting the coefficients of a Lasso Regression model involves understanding the impact of each feature on the target variable, considering the regularization effects of L1 penalty. Here are the key points to consider when interpreting the coefficients:\n",
    "\n",
    "1. **Non-Zero Coefficients:**\n",
    "   - In Lasso Regression, the L1 penalty encourages sparsity by driving some coefficients exactly to zero. Features with non-zero coefficients are considered selected by the model and are deemed important in predicting the target variable.\n",
    "\n",
    "2. **Sign of Coefficients:**\n",
    "   - The sign of a non-zero coefficient indicates the direction of the relationship between the corresponding feature and the target variable. A positive coefficient suggests a positive relationship, while a negative coefficient suggests a negative relationship.\n",
    "\n",
    "3. **Magnitude of Coefficients:**\n",
    "   - The magnitude of the non-zero coefficients represents the strength of the relationship between the feature and the target variable. Larger magnitudes suggest a stronger impact on the target variable.\n",
    "\n",
    "4. **Relative Importance:**\n",
    "   - Comparing the magnitudes of different non-zero coefficients can provide insights into the relative importance of the corresponding features. Features with larger magnitudes generally have a greater influence on the model's predictions.\n",
    "\n",
    "5. **Zero Coefficients:**\n",
    "   - Features with coefficients set to zero by Lasso are essentially excluded from the model. This implies that the model considers these features as less relevant or even irrelevant in predicting the target variable.\n",
    "\n",
    "6. **Sparsity and Feature Selection:**\n",
    "   - Lasso's primary advantage is its ability to perform automatic feature selection by driving some coefficients to zero. This sparsity facilitates a simpler and more interpretable model, focusing on the most important features.\n",
    "\n",
    "7. **Regularization Strength (Alpha):**\n",
    "   - The regularization strength (alpha) in Lasso Regression determines the trade-off between fitting the training data well and keeping the model simple. Higher values of alpha result in more coefficients being pushed to zero, leading to a sparser model.\n",
    "\n",
    "8. **Interaction Effects:**\n",
    "   - Interpretation becomes more complex in the presence of interaction effects between features. Understanding the impact of one feature may depend on the values of other features in the model.\n",
    "\n",
    "It's crucial to note that interpreting coefficients in any regression model, including Lasso Regression, requires caution. Correlation does not imply causation, and the identified relationships should be interpreted within the context of the specific dataset and domain knowledge. Additionally, the interpretation of coefficients becomes more challenging when dealing with high-dimensional datasets and collinear features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb86c37-f4aa-4038-81a6-264e7425fb71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "546a75ef-1005-4aa7-be6a-643ab9ec364c",
   "metadata": {},
   "source": [
    "In Lasso Regression, the main tuning parameter is the regularization strength, often denoted as \\(\\alpha\\). This parameter controls the balance between fitting the training data well and keeping the model simple by penalizing the absolute values of the coefficients. The L1 regularization term in the objective function is \\(\\alpha \\times \\sum_{i=1}^{n} |\\beta_i|\\), where \\(\\beta_i\\) represents the coefficients of the features.\n",
    "\n",
    "Here's how the tuning parameter \\(\\alpha\\) affects the model's performance:\n",
    "\n",
    "1. **\\(\\alpha = 0\\):**\n",
    "   - When \\(\\alpha\\) is set to zero, Lasso Regression becomes equivalent to Ordinary Least Squares (OLS) regression. The model aims to minimize the sum of squared errors without any regularization. This may lead to overfitting, especially in situations where the number of features is much larger than the number of observations.\n",
    "\n",
    "2. **Small \\(\\alpha\\):**\n",
    "   - As \\(\\alpha\\) increases from zero, the regularization term becomes more influential. This encourages sparsity in the model, leading to some coefficients being exactly zero. Small \\(\\alpha\\) values provide less regularization, and the model's performance may be similar to OLS.\n",
    "\n",
    "3. **Intermediate \\(\\alpha\\):**\n",
    "   - Intermediate values of \\(\\alpha\\) strike a balance between fitting the training data well and penalizing the absolute values of the coefficients. This can result in a model with some coefficients exactly equal to zero, leading to feature selection and a simpler model.\n",
    "\n",
    "4. **Large \\(\\alpha\\):**\n",
    "   - When \\(\\alpha\\) is set to a large value, the regularization term dominates the objective function. This results in more coefficients being driven to zero, and the model becomes increasingly sparse. Large \\(\\alpha\\) values can help prevent overfitting, especially in situations where there are many irrelevant or redundant features.\n",
    "\n",
    "5. **Cross-Validation for \\(\\alpha\\) Selection:**\n",
    "   - Determining the optimal value of \\(\\alpha\\) is typically done through cross-validation. Common cross-validation techniques, such as k-fold cross-validation, can be employed to assess the model's performance for different \\(\\alpha\\) values. The value of \\(\\alpha\\) that results in the best performance (e.g., minimizing mean squared error) on the validation set is often chosen.\n",
    "\n",
    "6. **Grid Search or Random Search:**\n",
    "   - Practitioners often perform a grid search or random search over a range of \\(\\alpha\\) values to find the optimal one. This involves training and evaluating Lasso Regression models with different \\(\\alpha\\) values and selecting the one that maximizes model performance on the validation set.\n",
    "\n",
    "It's important to note that the choice of \\(\\alpha\\) depends on the specific characteristics of the dataset. In some cases, researchers and data scientists may use domain knowledge or additional information to guide the selection of the regularization strength. Additionally, in situations where both L1 and L2 regularization are desired, Elastic Net Regression, which combines both penalties, can be used with additional tuning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a108cb82-574f-4dd1-bcb5-b3d3bd94602a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "045f535b-098f-429e-acce-0c5a1df52ae5",
   "metadata": {},
   "source": [
    "Lasso Regression is inherently a linear regression technique, meaning it is designed to model linear relationships between the input features and the target variable. However, it is possible to extend Lasso Regression to handle non-linear regression problems by incorporating non-linear transformations of the features.\n",
    "\n",
    "Here are a few approaches to use Lasso Regression for non-linear regression problems:\n",
    "\n",
    "1. **Feature Engineering:**\n",
    "   - One common method is to engineer new features by applying non-linear transformations to the original features. For example, you can introduce polynomial features by squaring or cubing existing features. These polynomial features allow the model to capture non-linear relationships.\n",
    "\n",
    "2. **Interaction Terms:**\n",
    "   - Include interaction terms between features. This involves multiplying two or more features to create new features that capture interactions. The interaction terms can help model non-linear relationships between the features and the target variable.\n",
    "\n",
    "3. **Kernelized Regression:**\n",
    "   - Another approach is to use kernelized regression techniques, such as the kernel trick in Support Vector Machines (SVMs) or kernelized ridge regression. Kernels enable the model to implicitly operate in a higher-dimensional space, capturing non-linear relationships without explicitly computing the transformed features.\n",
    "\n",
    "4. **Gaussian Radial Basis Functions (RBF):**\n",
    "   - Utilize radial basis functions as features. RBFs are often used in combination with Lasso Regression to model non-linear relationships. Each data point is represented by a set of RBFs centered at different points.\n",
    "\n",
    "5. **Splines and Piecewise Linear Functions:**\n",
    "   - Represent non-linear relationships using splines or piecewise linear functions. Break the input range into segments and fit linear functions within each segment. This approach is effective for capturing local non-linear patterns.\n",
    "\n",
    "6. **Ensemble Models:**\n",
    "   - Combine Lasso Regression with ensemble methods, such as Random Forests or Gradient Boosting, which inherently handle non-linear relationships. Use Lasso Regression as one of the base models within the ensemble.\n",
    "\n",
    "It's essential to note that while these approaches extend Lasso Regression to handle non-linearities, they also introduce additional complexity to the model. Care should be taken to avoid overfitting, and regularization parameters (such as \\(\\alpha\\) in Lasso) may need to be carefully tuned. Cross-validation can help assess the model's performance and identify the optimal regularization parameters for the specific non-linear regression problem.\n",
    "\n",
    "If the non-linearities in the data are complex, other regression techniques specifically designed for non-linear relationships, such as decision trees, random forests, or neural networks, might be more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d1fe2-baf8-4c6f-8c20-6e370daf1f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b2e7fe2-49a3-46a0-9c8c-7b0ed6f30805",
   "metadata": {},
   "source": [
    "Ridge Regression and Lasso Regression are both linear regression techniques with a regularization term added to the cost function. However, they differ in the type of regularization used and, consequently, in their impact on the model. Here are the key differences between Ridge Regression and Lasso Regression:\n",
    "\n",
    "1. **Regularization Term:**\n",
    "   - **Ridge Regression:**\n",
    "     - Uses an L2 regularization term, which adds the squared values of the coefficients to the cost function. The regularization term is \\(\\alpha \\times \\sum_{i=1}^{n} \\beta_i^2\\), where \\(\\alpha\\) is the regularization parameter and \\(\\beta_i\\) represents the coefficients of the features.\n",
    "   - The L2 penalty tends to shrink all the coefficients towards zero but rarely makes them exactly zero.\n",
    "\n",
    "   - **Lasso Regression:**\n",
    "     - Uses an L1 regularization term, which adds the absolute values of the coefficients to the cost function. The regularization term is \\(\\alpha \\times \\sum_{i=1}^{n} |\\beta_i|\\).\n",
    "     - The L1 penalty encourages sparsity in the model by driving some coefficients exactly to zero. This results in automatic feature selection.\n",
    "\n",
    "2. **Sparsity:**\n",
    "   - **Ridge Regression:**\n",
    "     - Does not typically lead to sparsity. The coefficients are reduced in magnitude but rarely become exactly zero. Ridge regression is effective when dealing with multicollinearity among features.\n",
    "\n",
    "   - **Lasso Regression:**\n",
    "     - Induces sparsity in the model. Some coefficients are exactly set to zero, leading to automatic feature selection. Lasso is particularly useful when there is a need to identify and select a subset of important features.\n",
    "\n",
    "3. **Impact on Coefficients:**\n",
    "   - **Ridge Regression:**\n",
    "     - Tends to shrink the coefficients towards zero, reducing their magnitudes. All features may still contribute to the model, but their impact is diminished.\n",
    "\n",
    "   - **Lasso Regression:**\n",
    "     - Can drive some coefficients to exactly zero, effectively excluding certain features from the model. This leads to a sparse model with fewer features.\n",
    "\n",
    "4. **Handling Multicollinearity:**\n",
    "   - **Ridge Regression:**\n",
    "     - Particularly useful in handling multicollinearity, as it does not eliminate any features but reduces their impact.\n",
    "\n",
    "   - **Lasso Regression:**\n",
    "     - Automatically selects one feature from a group of highly correlated features and sets the others to zero. This helps address multicollinearity by choosing a subset of features.\n",
    "\n",
    "5. **Objective Function:**\n",
    "   - **Ridge Regression:**\n",
    "     - Minimizes the sum of squared errors plus the squared magnitudes of the coefficients.\n",
    "\n",
    "   - **Lasso Regression:**\n",
    "     - Minimizes the sum of squared errors plus the absolute values of the coefficients.\n",
    "\n",
    "6. **Cross-Validation for Parameter Selection:**\n",
    "   - Both Ridge and Lasso Regression often use cross-validation to select the optimal regularization parameter (\\(\\alpha\\)) by assessing the model's performance on a validation set.\n",
    "\n",
    "In summary, while both Ridge and Lasso Regression introduce regularization to prevent overfitting, they differ in the type of penalty they apply and their impact on the model's coefficients. Ridge tends to shrink coefficients towards zero without eliminating any, while Lasso can lead to a sparse model with some coefficients being exactly zero, effectively performing feature selection. The choice between Ridge and Lasso depends on the specific characteristics of the dataset and the goals of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e2d7c-1aeb-4b16-888c-d0e65cb4a505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b71bfd76-5037-4bdc-8f27-aa2d889a9a3a",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression has a property that can help address multicollinearity in the input features. Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, which can lead to instability in the estimated coefficients. Lasso Regression can be useful in handling multicollinearity through its ability to perform feature selection.\n",
    "\n",
    "Here's how Lasso Regression can address multicollinearity:\n",
    "\n",
    "1. **Automatic Feature Selection:**\n",
    "   - Lasso Regression's L1 regularization term includes the absolute values of the coefficients (\\(\\alpha \\times \\sum_{i=1}^{n} |\\beta_i|\\)). During the optimization process, Lasso tends to drive some coefficients exactly to zero.\n",
    "\n",
    "2. **Sparse Model:**\n",
    "   - When multicollinearity is present, highly correlated features may carry redundant information. Lasso has the ability to select one feature from a group of correlated features and set the others to zero.\n",
    "\n",
    "3. **Subset of Important Features:**\n",
    "   - Lasso's feature selection property results in a sparse model with only a subset of the input features having non-zero coefficients. These selected features are deemed most important by the model.\n",
    "\n",
    "4. **Collinear Features Assigned Coefficients:**\n",
    "   - Lasso tends to assign non-zero coefficients to one or a few features in a group of correlated features. The features with non-zero coefficients are retained in the model, and the others are effectively excluded.\n",
    "\n",
    "5. **Regularization Parameter (\\(\\alpha\\)):**\n",
    "   - The strength of the feature selection is controlled by the regularization parameter (\\(\\alpha\\)). A higher \\(\\alpha\\) value increases the penalty on the absolute values of the coefficients, leading to more coefficients being pushed to zero.\n",
    "\n",
    "6. **Balance Between Fitting and Simplicity:**\n",
    "   - Lasso Regression strikes a balance between fitting the training data well and keeping the model simple. By favoring sparsity, it helps address multicollinearity by automatically selecting a subset of relevant features.\n",
    "\n",
    "It's important to note that while Lasso Regression can be effective in handling multicollinearity, the choice between Lasso and other techniques like Ridge Regression or Elastic Net depends on the specific characteristics of the dataset. Ridge Regression, which uses an L2 regularization term, is also effective in reducing the impact of correlated features but does not lead to feature selection. In some cases, a combination of L1 and L2 regularization in Elastic Net Regression may provide a good compromise, especially when there are groups of correlated features. Cross-validation is often employed to select the optimal regularization parameter and assess the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b556fa3f-a855-4fcd-b482-9fc853557f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a26319c6-89d8-4052-9209-1aa559ea9052",
   "metadata": {},
   "source": [
    "Choosing the optimal value of the regularization parameter (\\(\\alpha\\), often referred to as lambda) in Lasso Regression is a critical step in building an effective model. The goal is to find the right balance between fitting the training data well and preventing overfitting by controlling the sparsity of the model. Cross-validation is commonly used to select the optimal \\(\\alpha\\) value. Here's a general approach:\n",
    "\n",
    "1. **Grid Search:**\n",
    "   - Define a range of \\(\\alpha\\) values to test. This range should cover a spectrum from very small values (virtually no regularization) to large values (strong regularization). Commonly used values are logarithmically spaced.\n",
    "\n",
    "2. **Cross-Validation:**\n",
    "   - Split the dataset into training and validation sets. The training set is used to train the model, and the validation set is used to evaluate the model's performance for different \\(\\alpha\\) values.\n",
    "\n",
    "3. **Train and Evaluate:**\n",
    "   - For each \\(\\alpha\\) value in the predefined range, train the Lasso Regression model using the training set and evaluate its performance on the validation set. The evaluation metric can be chosen based on the specific problem (e.g., mean squared error for regression, accuracy for classification).\n",
    "\n",
    "4. **Select Optimal \\(\\alpha\\):**\n",
    "   - Choose the \\(\\alpha\\) value that results in the best performance on the validation set. This is typically the \\(\\alpha\\) that minimizes the chosen evaluation metric. It represents the optimal trade-off between bias and variance.\n",
    "\n",
    "5. **Test Set (Optional):**\n",
    "   - Optionally, if you have a separate test set that was not used during model selection, you can further evaluate the model's performance on this set to ensure that the chosen \\(\\alpha\\) generalizes well to new, unseen data.\n",
    "\n",
    "6. **Cross-Validation Techniques:**\n",
    "   - Common cross-validation techniques include k-fold cross-validation and leave-one-out cross-validation. In k-fold cross-validation, the dataset is divided into k folds, and the model is trained and evaluated k times, with each fold serving as the validation set once.\n",
    "\n",
    "7. **Nested Cross-Validation (Optional):**\n",
    "   - To obtain a more robust estimate of model performance and better assess how well the chosen \\(\\alpha\\) generalizes, you can use nested cross-validation. In this approach, an outer loop of cross-validation is used for model evaluation, and an inner loop is used for \\(\\alpha\\) selection.\n",
    "\n",
    "8. **Automated Methods (Optional):**\n",
    "   - Some libraries and tools provide automated methods for hyperparameter tuning, such as scikit-learn's `GridSearchCV` or `RandomizedSearchCV`. These tools perform grid search or random search over specified parameter ranges and cross-validate the model for each combination of parameters.\n",
    "\n",
    "9. **Visualization (Optional):**\n",
    "   - Optionally, you can visualize the relationship between \\(\\alpha\\) values and the model's performance using plots or graphs. This can help you understand how the model's complexity changes with different regularization strengths.\n",
    "\n",
    "Keep in mind that the optimal \\(\\alpha\\) may vary depending on the specific dataset and problem. It's a good practice to repeat the process on multiple datasets or using different splits to ensure the robustness of the chosen regularization parameter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
